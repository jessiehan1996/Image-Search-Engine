{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jintao Lai (jl3837), Jessie Han, Jiaxin Guo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Terry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Terry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Terry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/Terry/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Terry/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import wordnet \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import Lasso\n",
    "from scipy import sparse\n",
    "from heapq import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "training_set = 10000\n",
    "test_set = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_stamp():\n",
    "    now = int(time.time()) \n",
    "    timeArray = time.localtime()\n",
    "    otherStyleTime = time.strftime(\"%Y-%m-%d_%H-%M-%S\", timeArray)\n",
    "    print otherStyleTime\n",
    "    return otherStyleTime\n",
    "\n",
    "from sklearn.preprocessing import normalize, FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "def postprocessing(data):\n",
    "#     normalize(data, norm='max', copy=False)\n",
    "    data = transformer.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "def get_tags(train_or_test):\n",
    "    size = None\n",
    "    path = None\n",
    "    \n",
    "    if train_or_test == \"train\":\n",
    "        size = training_set\n",
    "        path = 'tags_train/'\n",
    "    elif  train_or_test == \"test\":\n",
    "        size = test_set\n",
    "        path = 'tags_test/' \n",
    "        \n",
    "\n",
    "    tags = []\n",
    "    for i in range(size):\n",
    "        data = open(path + str(i) + '.txt')\n",
    "        lines = data.readlines()\n",
    "        temp = \"\"\n",
    "        for line in lines:\n",
    "            word = line.split(':')[1].replace(' ', '').split('\\n')[0]\n",
    "            temp+= (word+\" \")\n",
    "\n",
    "        tags.append(temp[0:-1])\n",
    "    \n",
    "    return tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "def get_tags(train_or_test):\n",
    "    size = None\n",
    "    path = None\n",
    "    \n",
    "    if train_or_test == \"train\":\n",
    "        size = training_set\n",
    "        path = 'tags_train/'\n",
    "    elif  train_or_test == \"test\":\n",
    "        size = test_set\n",
    "        path = 'tags_test/' \n",
    "        \n",
    "\n",
    "    tags = []\n",
    "    for i in range(size):\n",
    "        data = open(path + str(i) + '.txt')\n",
    "        lines = data.readlines()\n",
    "        temp = \"\"\n",
    "        for line in lines:\n",
    "            word = line.split(':')[1].replace(' ', '').split('\\n')[0]\n",
    "            temp+= (word+\" \")\n",
    "\n",
    "        tags.append(temp[0:-1])\n",
    "    \n",
    "    return tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_tag = get_tags(\"train\")\n",
    "test_tag =  get_tags(\"test\")\n",
    "\n",
    "#print train_tag\n",
    "# 空格\n",
    "tag_vectorizer = CountVectorizer(stop_words='english',analyzer='word', ngram_range=(1, 1))\n",
    "tag_vectorizer.fit(train_tag)\n",
    "\n",
    "train_tag_vectors = tag_vectorizer.transform(train_tag)\n",
    "print len(tag_vectorizer.vocabulary_)\n",
    "\n",
    "test_tag_vectors = tag_vectorizer.transform(test_tag)\n",
    "\n",
    "training_lables = train_tag_vectors.toarray().tolist()\n",
    "test_lables = test_tag_vectors.toarray().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dictionary_description = {}\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def get_all_lines(train_or_test, word_tag):\n",
    "    \n",
    "    size = None\n",
    "    path = None\n",
    "    \n",
    "    if train_or_test == \"train\":\n",
    "        size = training_set\n",
    "        path = 'descriptions_train/'\n",
    "    elif  train_or_test == \"test\":\n",
    "        size = test_set\n",
    "        path = 'descriptions_test/' \n",
    "        \n",
    "    i = 0\n",
    "    all_lines =[]\n",
    "    for i in range(size):\n",
    "        data = open(path + str(i) + '.txt')\n",
    "        lines = data.readlines()\n",
    "        words = []\n",
    "        for line in lines:\n",
    "            text_list = nltk.word_tokenize(line)\n",
    "            english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "            text_list = [word for word in text_list if word not in english_punctuations]\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            text_list = [word for word in text_list if word not in stops]\n",
    "            text_list = nltk.pos_tag(text_list)\n",
    "            \n",
    "            word_set = None\n",
    "            NN = set([\"NN\",\"NNS\",\"NNPS\",\"NNP\"])\n",
    "            JJ = set([\"JJ\",\"JJR\",\"JJS\"])\n",
    "            VV = set([\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"])\n",
    "            \n",
    "            if word_tag == 1:\n",
    "                word_set =  NN | JJ\n",
    "            elif word_tag == 2:\n",
    "                word_set = NN | JJ | VV\n",
    "                #word_set =  JJ | VV\n",
    "            else:\n",
    "                word_set = NN\n",
    "                \n",
    "            for word,tag in text_list:\n",
    "                if tag in word_set:\n",
    "                    words.append(stemmer.stem(word))\n",
    "\n",
    "        if i == 0:\n",
    "            print words\n",
    "\n",
    "        all_lines.append(\" \".join(words))\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'skateboard', u'show', u'picnic', u'tabl', u'stage', u'skateboard', u'trick', u'top', u'picnic', u'tabl', u'man', u'skateboard', u'top', u'tabl', u'skate', u'boarder', u'trick', u'picnic', u'tabl', u'person', u'skateboard', u'picnic', u'tabl', u'crowd', u'watch']\n",
      "skateboard show picnic tabl stage skateboard trick top picnic tabl man skateboard top tabl skate boarder trick picnic tabl person skateboard picnic tabl crowd watch\n",
      "[u'woman', u'street', u'doorway', u'woman', u'past', u'doorway', u'sidewalk', u'woman', u'cell', u'phone', u'check', u'woman', u'talk', u'phone', u'check', u'sidewalk', u'woman', u'blue', u'phone', u'sidewalk', u'front', u'build', u'black', u'planter', u'side', u'entranc']\n",
      "woman street doorway woman past doorway sidewalk woman cell phone check woman talk phone check sidewalk woman blue phone sidewalk front build black planter side entranc\n"
     ]
    }
   ],
   "source": [
    "all_lines = get_all_lines(\"train\",1)\n",
    "print(all_lines[0])\n",
    "\n",
    "all_lines_test = get_all_lines(\"test\",1)\n",
    "print all_lines_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'skateboard', u'put', u'show', u'use', u'picnic', u'tabl', u'stage', u'skateboard', u'pull', u'trick', u'top', u'picnic', u'tabl', u'man', u'ride', u'skateboard', u'top', u'tabl', u'skate', u'boarder', u'trick', u'picnic', u'tabl', u'person', u'ride', u'skateboard', u'picnic', u'tabl', u'crowd', u'watch']\n",
      "skateboard put show use picnic tabl stage skateboard pull trick top picnic tabl man ride skateboard top tabl skate boarder trick picnic tabl person ride skateboard picnic tabl crowd watch\n",
      "[u'woman', u'walk', u'street', u'doorway', u'woman', u'walk', u'past', u'doorway', u'sidewalk', u'woman', u'talk', u'cell', u'phone', u'check', u'watch', u'woman', u'talk', u'cell', u'phone', u'check', u'watch', u'walk', u'sidewalk', u'woman', u'wear', u'blue', u'phone', u'walk', u'sidewalk', u'front', u'build', u'black', u'planter', u'side', u'entranc']\n",
      "woman walk street doorway woman walk past doorway sidewalk woman talk cell phone check watch woman talk cell phone check watch walk sidewalk woman wear blue phone walk sidewalk front build black planter side entranc\n"
     ]
    }
   ],
   "source": [
    "all_lines_p = get_all_lines(\"train\",2)\n",
    "print(all_lines_p[0])\n",
    "\n",
    "all_lines_test_p = get_all_lines(\"test\",2)\n",
    "print all_lines_test_p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 8, stop_words='english',analyzer='word', ngram_range=(1, 1))\n",
    "vectorizer_p = CountVectorizer(min_df = 8, stop_words='english',analyzer='word', ngram_range=(1, 1))\n",
    "\n",
    "vectorizer.fit(all_lines)\n",
    "train_vectors = vectorizer.transform(all_lines)\n",
    "\n",
    "\n",
    "vectorizer_p.fit(all_lines_p)\n",
    "train_vectors_p = vectorizer_p.transform(all_lines_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 1452\n",
      "features: 1664\n"
     ]
    }
   ],
   "source": [
    "print \"features: \" + str(len(vectorizer.vocabulary_))\n",
    "print \"features: \" + str(len(vectorizer_p.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = vectorizer.transform(all_lines_test)\n",
    "test_vectors_p = vectorizer_p.transform(all_lines_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FC1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1000_train_raw = pd.read_csv(filepath_or_buffer=\"./features_train/features_resnet1000_train.csv\",header=None)\n",
    "fc1000_train_raw[0] = fc1000_train_raw[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "fc1000_train_raw_sorted = fc1000_train_raw.sort_values(by=[0])\n",
    "fc1000_train_img = fc1000_train_raw_sorted[0].values\n",
    "fc1000_train = fc1000_train_raw_sorted[list(range(1,1001))].values\n",
    "\n",
    "fc1000_test_raw = pd.read_csv(filepath_or_buffer=\"./features_test/features_resnet1000_test.csv\",header=None)\n",
    "fc1000_test_raw[0] = fc1000_test_raw[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "fc1000_test_raw_sorted = fc1000_test_raw.sort_values(by=[0])\n",
    "fc1000_test_img = fc1000_test_raw_sorted[0].values\n",
    "fc1000_test = fc1000_test_raw_sorted[list(range(1,1001))].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pool5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool5_train_raw = pd.read_csv(filepath_or_buffer=\"./features_train/features_resnet1000intermediate_train.csv\",header=None)\n",
    "pool5_train_raw[0] = pool5_train_raw[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "pool5_train_raw_sorted = pool5_train_raw.sort_values(by=[0])\n",
    "pool5_train_img = pool5_train_raw_sorted[0].values\n",
    "pool5_train = pool5_train_raw_sorted[list(range(1,2049))].values\n",
    "\n",
    "pool5_test_raw = pd.read_csv(filepath_or_buffer=\"./features_test/features_resnet1000intermediate_test.csv\",header=None)\n",
    "pool5_test_raw[0] = pool5_test_raw[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "pool5_test_raw_sorted = pool5_test_raw.sort_values(by=[0])\n",
    "pool5_test_img = pool5_test_raw_sorted[0].values\n",
    "pool5_test = pool5_test_raw_sorted[list(range(1,2049))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2048)\n",
      "(2000, 2048)\n"
     ]
    }
   ],
   "source": [
    "print pool5_train.shape\n",
    "print pool5_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fc1000_train\n",
    "train_lable = postprocessing(train_vectors.toarray())\n",
    "predict_data  = fc1000_test\n",
    "test_lable = postprocessing(test_vectors.toarray())\n",
    "NN_input_1 = test_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06_18-13-48\n",
      "start: 2018-12-06_18-13-48\n",
      "2018-12-06_18-13-50\n",
      "end: 2018-12-06_18-13-50\n"
     ]
    }
   ],
   "source": [
    "predict_tags = []\n",
    "print \"start: \" + get_time_stamp()\n",
    "clf_1 = Ridge(alpha = 30, tol=0.0001)\n",
    "clf_1.fit(train_data, train_lable)\n",
    "pre_result_1 = clf_1.predict(predict_data)\n",
    "print \"end: \" + get_time_stamp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pool5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_p = pool5_train\n",
    "train_lable_p = postprocessing(train_vectors_p.toarray())\n",
    "predict_data_p  = pool5_test\n",
    "test_lable_p = postprocessing(test_vectors_p.toarray())\n",
    "NN_input_2 = test_lable_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06_19-11-22\n",
      "start: 2018-12-06_19-11-22\n",
      "2018-12-06_19-11-25\n",
      "end: 2018-12-06_19-11-25\n"
     ]
    }
   ],
   "source": [
    "print \"start: \" + get_time_stamp()\n",
    "#normalize=True\n",
    "\n",
    "#rcv.fit(train_data_p, train_lable_p)\n",
    "clf_2 = Ridge(alpha = 100)\n",
    "\n",
    "clf_2.fit(train_data_p, train_lable_p)\n",
    "pre_result_2 = clf_2.predict(predict_data_p)\n",
    "print \"end: \" + get_time_stamp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_t = postprocessing(train_vectors.toarray())\n",
    "train_lable_t = train_tag_vectors.toarray()\n",
    "predict_data_t  = postprocessing(test_vectors.toarray())\n",
    "test_lable_t = test_tag_vectors.toarray()\n",
    "NN_input_3 = test_lable_t\n",
    "train_data_t = sparse.csr_matrix(train_data_t)\n",
    "predict_data_t = sparse.csr_matrix(predict_data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-637d388fc648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lable_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_data_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredict_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Terry/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict_tags = []\n",
    "for i in range(len(tag_vectorizer.vocabulary_)):\n",
    "    #print str(i) + \": \" + get_time_stamp()\n",
    "    y = train_lable_t[:,i]\n",
    "    clf = RandomForestRegressor(random_state=0, n_estimators=5, max_depth = 10)\n",
    "    clf.fit(train_data_t, y)\n",
    "    tag = clf.predict(predict_data_t)\n",
    "    predict_tags.append(tag)\n",
    "predict_tags_trans = np.array(predict_tags).T\n",
    "pre_result_3 = predict_tags_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def get_distacne_top_20_and_value(NN_input,pre_result,numbers,metric_method):\n",
    "    #euclidean cosine jaccard\n",
    "    distance_matrix = sklearn.metrics.pairwise_distances(NN_input,pre_result,metric = metric_method)\n",
    "    # bb = sklearn.metrics.pairwise_distances(pre_result_2,NN_input_2,metric='euclidean')\n",
    "    all_rank = []\n",
    "    all_scores = []\n",
    "    for line in distance_matrix:\n",
    "        all_rank_row = []\n",
    "        all_rank_scores = []\n",
    "        heap =[]\n",
    "        for key,value in enumerate(line):\n",
    "            heappush(heap,(value,key))\n",
    "        for i in range(numbers):\n",
    "            value, key = heappop(heap)\n",
    "            all_rank_row.append(key)\n",
    "            all_rank_scores.append(value)\n",
    "        all_rank.append(all_rank_row)\n",
    "        all_scores.append(all_rank_scores)\n",
    "    return all_rank, all_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cos_1,cosines_1 = get_distacne_top_20_and_value(NN_input_1, pre_result_1,20,'cosine')\n",
    "indices_l2_1,l2_1 = get_distacne_top_20_and_value(NN_input_1, pre_result_1,20,'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cos_2,cosines_2 = get_distacne_top_20_and_value(NN_input_2, pre_result_2,20,'cosine')\n",
    "indices_l2_2,l2_2 = get_distacne_top_20_and_value(NN_input_2, pre_result_2,20,'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cos_3,cosines_3 = get_distacne_top_20_and_value(NN_input_3, pre_result_3,20,'cosine')\n",
    "indices_l2_3,l2_3 = get_distacne_top_20_and_value(NN_input_3, pre_result_3,20,'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cos_rank(indices_cos_1,cosines_1,indices_cos_2,cosines_2, indices_cos_3,cosines_3):\n",
    "    ret = []\n",
    "\n",
    "    for i in range(len(indices_cos_1)):\n",
    "        scores = {}\n",
    "        cosines_1[i] = cosines_1[i] / np.linalg.norm(cosines_1[i])\n",
    "        cosines_2[i] = cosines_2[i] / np.linalg.norm(cosines_2[i])\n",
    "        cosines_3[i] = cosines_3[i] / np.linalg.norm(cosines_3[i])\n",
    "\n",
    "        for j in range(len(indices_cos_1[0])):\n",
    "            img1 = indices_cos_1[i][j]\n",
    "            img2 = indices_cos_2[i][j]\n",
    "            img3 = indices_cos_3[i][j]\n",
    "            \n",
    "            scores[img1] = (scores.get(img1,0) +  30*(1-cosines_1[i][j]))\n",
    "            scores[img2] = (scores.get(img2,0) +  100* (1-cosines_2[i][j]))\n",
    "            scores[img3] = (scores.get(img3,0) +  22*(1-cosines_3[i][j]))\n",
    "   \n",
    "                \n",
    "        row_ret = []\n",
    "        heap =[]\n",
    "        #print scores\n",
    "        for key, val in scores.items():\n",
    "            heappush(heap,(-val,key))\n",
    "        \n",
    "        for k in range(20):\n",
    "            val, key= heappop(heap)\n",
    "\n",
    "            #print key\n",
    "            if key not in row_ret:\n",
    "                row_ret.append(key)\n",
    "        ret.append(row_ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_ensembling = get_average_cos_rank(indices_cos_1,cosines_1,indices_cos_2,cosines_2, indices_cos_3,cosines_3)\n",
    "print indices_ensembling[0]\n",
    "print indices_cos_1[0]\n",
    "print indices_cos_2[0]\n",
    "print indices_cos_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross Validation\n",
    "# def Three_Fold_Cross_Validation(train_set,labels):\n",
    "#     print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')+\" Start Three_Fold_Cross_Validation\")\n",
    "#     k_fold = KFold(len(train_set),n_folds=3, shuffle=True)\n",
    "#     accuracy = []\n",
    "#     show_confusion_matrix_flag = True\n",
    "#     for train_indices, test_indices in k_fold:\n",
    "#         train = train_set[train_indices]\n",
    "#         lable = labels[train_indices]\n",
    "#         test  = train_set[test_indices]\n",
    "#         validate = labels[test_indices]\n",
    "#         result = []\n",
    "#         for i in range(len(test_indices)):\n",
    "#             result.append(KNN_Classifier(k_for_knn,train,lable,test[i]))\n",
    "#         count = 0\n",
    "#         for i in range(len(result)):\n",
    "#             if validate[i] == result[i]:\n",
    "#                 count+=1\n",
    "#         accuracy.append(count/len(result))\n",
    "        \n",
    "#         if show_confusion_matrix_flag:\n",
    "            \n",
    "#             show_confusion_matrix_flag = False\n",
    "#             cm = confusion_matrix(validate, result,labels=range(10))\n",
    "#             print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')+\" confusion_matrix\")\n",
    "#             print(cm)\n",
    "            \n",
    "        \n",
    "#     accuracy = np.array(accuracy)\n",
    "    \n",
    "    \n",
    "#     return accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(indices, model):\n",
    "    result_scv = []\n",
    "    for i in range(len(indices)):\n",
    "        result_string = \"\"\n",
    "        for j in indices[i]:\n",
    "            result_string+= (str(j)+\".jpg \")\n",
    "\n",
    "        string_temp = (str(i)+\".txt\",    result_string[0:len(result_string)-1])\n",
    "        result_scv.append(string_temp)\n",
    "\n",
    "    pd_data = pd.DataFrame(np.array(result_scv),columns=['Descritpion_ID','Top_20_Image_IDs'])\n",
    "    pd_data.to_csv(get_time_stamp() +'_'+model+'_.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(indices_ensembling,\"ensembling\")\n",
    "submission(indices_l2_1,\"fc1000\")\n",
    "submission(indices_l2_3,\"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06_19-11-32\n"
     ]
    }
   ],
   "source": [
    "submission(indices_l2_2,\"pool5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
